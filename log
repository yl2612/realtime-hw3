[cloudera@quickstart hw3]$ hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.5.0-mr1-cdh5.3.0.jar -mapper "python mapper.py" -reducer "python reducer.py" -input input3/ -output output3 -file mapper.py -file reducer.py
15/02/16 15:04:46 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.
packageJobJar: [mapper.py, reducer.py] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.5.0-cdh5.3.0.jar] /tmp/streamjob2012821765973198153.jar tmpDir=null
15/02/16 15:04:48 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
15/02/16 15:04:48 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
15/02/16 15:04:50 INFO mapred.FileInputFormat: Total input paths to process : 1
15/02/16 15:04:50 INFO mapreduce.JobSubmitter: number of splits:2
15/02/16 15:04:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1423416814719_0029
15/02/16 15:04:51 INFO impl.YarnClientImpl: Submitted application application_1423416814719_0029
15/02/16 15:04:51 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1423416814719_0029/
15/02/16 15:04:51 INFO mapreduce.Job: Running job: job_1423416814719_0029
15/02/16 15:04:59 INFO mapreduce.Job: Job job_1423416814719_0029 running in uber mode : false
15/02/16 15:04:59 INFO mapreduce.Job:  map 0% reduce 0%
15/02/16 15:05:08 INFO mapreduce.Job:  map 50% reduce 0%
15/02/16 15:05:09 INFO mapreduce.Job:  map 100% reduce 0%
15/02/16 15:05:16 INFO mapreduce.Job:  map 100% reduce 100%
15/02/16 15:05:16 INFO mapreduce.Job: Job job_1423416814719_0029 completed successfully
15/02/16 15:05:17 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=587
		FILE: Number of bytes written=328503
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=352
		HDFS: Number of bytes written=248
		HDFS: Number of read operations=9
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=2
		Launched reduce tasks=1
		Data-local map tasks=2
		Total time spent by all maps in occupied slots (ms)=14038
		Total time spent by all reduces in occupied slots (ms)=5175
		Total time spent by all map tasks (ms)=14038
		Total time spent by all reduce tasks (ms)=5175
		Total vcore-seconds taken by all map tasks=14038
		Total vcore-seconds taken by all reduce tasks=5175
		Total megabyte-seconds taken by all map tasks=14374912
		Total megabyte-seconds taken by all reduce tasks=5299200
	Map-Reduce Framework
		Map input records=6
		Map output records=21
		Map output bytes=539
		Map output materialized bytes=593
		Input split bytes=208
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=593
		Reduce input records=21
		Reduce output records=6
		Spilled Records=42
		Shuffled Maps =2
		Failed Shuffles=0
		Merged Map outputs=2
		GC time elapsed (ms)=167
		CPU time spent (ms)=2650
		Physical memory (bytes) snapshot=696848384
		Virtual memory (bytes) snapshot=2647953408
		Total committed heap usage (bytes)=465043456
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=144
	File Output Format Counters 
		Bytes Written=248
15/02/16 15:05:17 INFO streaming.StreamJob: Output directory: output3
[cloudera@quickstart hw3]$ hdfs dfs -ls /user/cloudera/output3
Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2015-02-16 15:05 /user/cloudera/output3/_SUCCESS
-rw-r--r--   1 cloudera cloudera        248 2015-02-16 15:05 /user/cloudera/output3/part-00000
[cloudera@quickstart hw3]$ hdfs dfs -cat /user/cloudera/output3/part-00000
A	["'C'", " 'F'", 0.1166669]
C	["'A'", " 'B'", 0.20000040000000002]
B	["'D'", " 'E'", " 'F'", 0.20000040000000002]
E	["'F'", 0.088889066666666669]
D	["'A'", " 'B'", " 'C'", " 'E'", " 'F'", 0.05555566666666667]
F	["'B'", " 'C'", 0.3388895666666667]
[cloudera@quickstart hw3]$ 
